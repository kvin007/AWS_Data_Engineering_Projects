{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glassdoor Reviews for Data Engineers\n",
    "### Data Engineering Capstone Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![title](images/data_engineer_title.png)\n",
    "\n",
    "\n",
    "#### Project Summary\n",
    "This project aims to answer the perception of current and past employees regarding Data Engineering roles in their companies. It initially consists of two data sources: one related to the reviews from the employees obtained from Glassdoor and the other dataset contains publicly available information regarding companies (such as the country of origin, the sector and industry)\n",
    "\n",
    "\n",
    "\n",
    "The project follows these steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "The project collects data from Glassdoor reviews in a specific format, and also data related to companies (for more analytical aggregations) in order to understand the perception of current and past employees regarding Data Engineering roles in their companies.\n",
    "\n",
    "The end solution proposed is a pipeline build upon Apache Airflow for ingesting data from Amazon S3 into Redshift, running transformations on Redshift and saving them into a proposed Star Schema.\n",
    "\n",
    "The end cases proposed are the following:\n",
    "- To serve any aspiring or current Data Engineer to get insights with summarized data regarding the field and companies.\n",
    "- To keep track of perception of the field over the years\n",
    "\n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "The project consists initially of 2 data sources:\n",
    "\n",
    "* Glassdoor Reviews: Data scraped from Glassdoor regarding the perception of employees on their companies. Freely avaliable on Kaggle (https://www.kaggle.com/datasets/davidgauthier/glassdoor-job-reviews)\n",
    "* Top companies data: Data obtained from Kaggle about the Forbes Top 2000 companies worlwide (https://www.kaggle.com/datasets/ash316/forbes-top-2000-companies) [Taking \"Forbes Top2000 2017.csv\" into account as it's the latest information available in the dataset]\n",
    "\n",
    "Both files use the CSV format. Though this project uses these files as a starting point, the main idea consists on having a separate process that scrapes Glassdoor reviews daily and inserts them into the corresponding bucket for having better insights over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data\n",
    "\n",
    "For the Glassdoor reviews dataset, we could identify the following data quality issues:\n",
    "* Companies name (firm on the dataset) have a hyphen character instead of a space character for separation.\n",
    "* Companies name ending with <b>'s</b> have a hypen before the <b>s</b> character\n",
    "* Job titles have extra spaces at the beginning of the string\n",
    "\n",
    "For the companies dataset, the data was pretty clean, though some columns contained missing values\n",
    "\n",
    "\n",
    "Please refer **inspection_notebook.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Cleaning Steps\n",
    "\n",
    "Since the transformation happens on Redshift, most the cleaning steps will be done using SQL\n",
    "\n",
    "For the Glassdoor reviews dataset:\n",
    "\n",
    "* The hyphens for the companies names are being handled using the REPLACE function, also for removing leading and trailing blanks the TRIM function is used\n",
    "* For the job titles, the INITCAP function is used to capitalize the first letter of each job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "The proposed data model consists of the following tables:\n",
    "\n",
    "* **Fact tables:**\n",
    "\n",
    "    - reviews (review_id, start_time, company_name, employee_role, number_positive_reviews, number_negative_reviews, number_total_reviews)\n",
    "\n",
    "* **Dimension tables:**\n",
    "    - companies (name, country, sector, industry)\n",
    "    - employee_roles (name, status)\n",
    "    - time (start_time, day, week, month, year, weekday)\n",
    "\n",
    "![title](images/logical_modeling_.png)\n",
    "\n",
    "The reasoning for using fact and dimension tables is that the reviews fact table in combination with the other dimension tables allow us to directly inspect or make aggregations regarding which companies had certain amount of reviews and the perception of employees based on the reviews.\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "The pipeline consists on the following steps:\n",
    "\n",
    "* Store the data into Amazon S3 to the corresponding key\n",
    "* Stage the data from S3 to Redshift\n",
    "* Perform cleaning and necessary transformations for storing it in the corresponding tables in the Star Schema\n",
    "* Do quality checks on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "\n",
    "Please refer the dags and plugins folder since it's an Airflow pipeline.\n",
    "The following pictures show the execution of the DAG and results in Amazon Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/dag_execution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/redshift_reviews.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example of an analytical query, the following  query is proposed to check the companies with best average for Data Engineers in 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/example_query.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "The quality checks performed mainly check the nullity on the reviews fact table\n",
    "\n",
    "* Check the number of positive reviews are not null\n",
    "* Check the number of negative reviews are not null\n",
    "* Check the number of total reviews are greater than zero\n",
    "* Check the dimensions are not null on the fact table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "The following data dictionary expressed each of the fields for the data modelling:\n",
    "![title](images/data_dictionary.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project\n",
    "    - The initial dataset is not that big (around 400MB) and can fit into most of modern day's computer. Apache Airflow allows us to schedule programmatically any kind of job and Redshift helps up maintain an informed schema while providing scaling capabilities\n",
    "\n",
    "* Propose how often the data should be updated and why.\n",
    "\n",
    "    * Depending on how frenquently you would like to inspect into Data Engineering jobs, you could feed data into the corresponding S3 buckets and generate the report. A weekly schedule should be enough for checking how the market and perception regarding certain companies is changing\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    "     - We would need to check if the performance provided by our Redshift instance is enough or consider upgrading it.\n",
    "     - Since Airflow is just a scheduler, we would not need to deal with the workers.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "     - I would need to set a scheduler interval for the pipeline to run at 5am with an SLA of 1 hour. In case this fails, it would trigger an alert and give us enough time to troubleshoot\n",
    " * The database needed to be accessed by 100+ people.\n",
    "     - Redshift gives us enough capabilities to handle that amount of load since it's considered a MPP database"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
